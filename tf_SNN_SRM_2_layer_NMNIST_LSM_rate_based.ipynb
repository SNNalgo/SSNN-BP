{"cells":[{"cell_type":"markdown","metadata":{"id":"soYRyTXKbfVX"},"source":["import stuff"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPzfqnebbRwH"},"outputs":[],"source":["import numpy as np\n","import tensorflow.compat.v1 as tf\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IHzjIyPX1QBm"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8XdBU6iggbG"},"outputs":[],"source":["import sys\n","sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks/SSNN-BP')"]},{"cell_type":"markdown","metadata":{"id":"GRq3ERFIl_x5"},"source":["tf.nn.depthwise_conv2d will be used to simulate the SNN with a Spike Response Model. For this, it needs input of shape (batch, 1, in_width, in_channels) and filters of shape (1, filter_width, in_channels, 1). The convolution occurs in the dimension of width - which should correspond to simulation steps\n","Describe the model in the following block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGxqiZc5pdtM"},"outputs":[],"source":["source_path = '/content/drive/MyDrive/Colab Notebooks/SSNN-BP/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9TvRLB0433Lf"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","X_train_load = np.load(source_path + 'train_LSM_out_NMNIST_p2.npy')\n","y_train_load = np.load(source_path + 'train_labels_NMNIST_p2.npy')\n","print(\"max = \", np.max(X_train_load))\n","print(\"min = \", np.min(X_train_load))\n","print(\"mean = \", np.mean(X_train_load))\n","\n","X_test_load = np.load(source_path + 'test_LSM_out_NMNIST_p2.npy')\n","y_test_load = np.load(source_path + 'test_labels_NMNIST_p2.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwGP-pEYDjL6"},"outputs":[],"source":["tf.disable_v2_behavior()\n","seed = 0\n","rng = np.random.RandomState(seed)\n","tf.set_random_seed(seed)\n","np.seterr(all='raise')\n","\n","batch_size = 100\n","# SNN parameters\n","\n","data_size = 60000\n","data_size_test = 8000\n","\n","num_iter = 51\n","max_in_spikes = 51\n","max_out_spikes = 25\n","boost_factor = 1\n","\n","X_train_load = boost_factor*X_train_load\n","X_train_load[X_train_load>1] = 1\n","\n","X_test_load = boost_factor*X_test_load\n","X_test_load[X_test_load>1] = 1\n","\n","print(\"total train size: \", X_train_load.shape[0])\n","print(\"total test size: \", X_test_load.shape[0])\n","print(\"train size: \", data_size)\n","print(\"test size: \", data_size_test)\n","\n","N = X_train_load.shape[1]\n","num_output = 10\n","\n","X_pl = tf.placeholder('float', [batch_size, 1, num_iter, N])\n","\n","X_data = X_train_load[:data_size]\n","y_data = y_train_load[:data_size]\n","\n","X_test = X_test_load[-data_size_test:]\n","#X_test = X_test + np.random.normal(0, 0.1, (X_test.shape[0], X_test.shape[1]))\n","y_test = y_test_load[-data_size_test:]\n","\n","# prepare the inputs\n","X_in_train = np.zeros((X_data.shape[0], 1, num_iter, X_data.shape[1]), dtype=np.int8)\n","y_in_train = np.zeros((X_data.shape[0], 1, num_iter, num_output), dtype=np.int8)\n","X_in_test = np.zeros((X_test.shape[0], 1, num_iter, X_test.shape[1]), dtype=np.int8)\n","y_in_test = np.zeros((X_test.shape[0], 1, num_iter, num_output), dtype=np.int8)\n","\n","for i in range(X_data.shape[0]):\n","    label_ind = np.int32(y_data[i])\n","    spikes = np.random.uniform(size=num_iter)\n","    spikes[spikes<(max_out_spikes/num_iter)] = 1\n","    spikes[spikes<1] = 0\n","    y_in_train[i,0,:,label_ind] = spikes\n","\n","for i in range(X_test.shape[0]):\n","    label_ind = np.int32(y_test[i])\n","    spikes = np.random.uniform(size=num_iter)\n","    spikes[spikes<(max_out_spikes/num_iter)] = 1\n","    spikes[spikes<1] = 0\n","    y_in_test[i,0,:,label_ind] = spikes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-NTZJA8ipq4"},"outputs":[],"source":["batch_size = 100\n","\n","X_in_train = np.zeros((X_data.shape[0], 1, num_iter, X_data.shape[1]), dtype=np.int8)\n","X_in_test = np.zeros((X_test.shape[0], 1, num_iter, X_test.shape[1]), dtype=np.int8)\n","\n","num_batches = data_size//batch_size\n","num_batches_test = data_size_test//batch_size\n","for b in range(num_batches):\n","  if(b%50==0):\n","    print(\"completed: \", b/num_batches)\n","  batch_data = X_data[b*batch_size:(b+1)*batch_size]*(max_in_spikes/num_iter)\n","  batch_reshp = np.reshape(batch_data, (batch_size, -1, 1, 1)) #shape=(batch_size, N, 1, 1)\n","  batch_data_tiled = np.tile(batch_reshp, (1, 1, num_iter, 1)) #shape=(batch_size, N, num_iter, 1)\n","  batch_data_trans = np.transpose(batch_data_tiled, (0,3,2,1)) #shape=(batch_size, 1, num_iter, N)\n","  batch_in = np.random.uniform(size=(batch_size, 1, num_iter, N))\n","  batch_in[batch_in<batch_data_trans] = 1\n","  batch_in[batch_in<1] = 0\n","  X_in_train[b*batch_size:(b+1)*batch_size] = batch_in\n","\n","for b in range(num_batches_test):\n","  batch_data = X_test[b*batch_size:(b+1)*batch_size]*(max_in_spikes/num_iter)\n","  batch_reshp = np.reshape(batch_data, (batch_size, -1, 1, 1))\n","  batch_data_tiled = np.tile(batch_reshp, (1, 1, num_iter, 1))\n","  batch_data_trans = np.transpose(batch_data_tiled, (0,3,2,1))\n","  batch_in = np.random.uniform(size=(batch_size, 1, num_iter, N))\n","  batch_in[batch_in<batch_data_trans] = 1\n","  batch_in[batch_in<1] = 0\n","  X_in_test[b*batch_size:(b+1)*batch_size] = batch_in"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZdFZEPRbcuJ"},"outputs":[],"source":["import RateSNN as snn\n","\n","num_epochs = 101\n","batch_size = 200\n","lr = 0.1\n","beta = 0.9\n","eps = 1e-8\n","k_size = num_iter\n","hidden_size = 1280\n","th_val_h = 5\n","th_val_y = 5\n","\n","self_iter = 5\n","#self_iter = 0\n","\n","# Initialize Placeholders (X_in, labels, dropout_masks, kernels)\n","keep_prob1 = tf.placeholder('float', shape=())\n","keep_prob2 = tf.placeholder('float', shape=())\n","tf_kernelh = tf.placeholder('float', [1, k_size, hidden_size, 1])\n","tf_kernelh_b = tf.placeholder('float', [1, k_size, hidden_size, 1])\n","tf_kernely = tf.placeholder('float', [1, k_size, num_output, 1])\n","tf_kernely_b = tf.placeholder('float', [1, k_size, num_output, 1])\n","dropout_mask1 = tf.placeholder('float', [batch_size,1,num_iter,N])\n","dropout_mask2 = tf.placeholder('float', [batch_size,1,num_iter,hidden_size])\n","X_pl = tf.placeholder('float', [batch_size, 1, num_iter, N])\n","s_label = tf.placeholder('float', [batch_size, 1, num_iter, num_output])\n","\n","# Initialize Weights (He initialization)\n","W1 = tf.Variable(tf.random_normal((N,hidden_size), stddev=np.sqrt(4/N)), trainable=False)\n","W2 = tf.Variable(tf.random_normal((hidden_size,num_output), stddev=np.sqrt(4/hidden_size)), trainable=False)\n","W_inh = tf.Variable(tf.matrix_set_diag(1.0*tf.ones((num_output, num_output)), tf.zeros(num_output)), trainable=False)\n","W1mt = tf.Variable(tf.zeros((N,hidden_size)), trainable=False)\n","W2mt = tf.Variable(tf.zeros((hidden_size, num_output)), trainable=False)\n","\n","W1mt_acc = tf.Variable(W1mt)\n","W2mt_acc = tf.Variable(W2mt)\n","\n","# MOMENTUM\n","#W1_acc = W1\n","#W2_acc = W2\n","\n","# NESTEROV ACCELERATED GRADIENT\n","W1_acc = W1 + beta*W1mt_acc\n","W2_acc = W2 + beta*W2mt_acc\n","\n","X_masked, s_out_h_acc = snn.forward_pass(X_pl, dropout_mask1, W1_acc, th_val_h, tf_kernelh)\n","s_out_h_masked, s_out_y_acc = snn.y_WTA(s_out_h_acc, dropout_mask2, W2_acc, th_val_y, tf_kernely, W_inh, self_iter)\n","\n","del_p_y, del_m_y, W2gt_orig = snn.finalW_update(s_out_y_acc, s_out_h_masked, s_label, th_val_y, tf_kernely_b)\n","del_p_h, del_m_h, W1gt_orig = snn.innerW_update(s_out_h_masked, X_masked, del_p_y, del_m_y, W2_acc, th_val_h, tf_kernelh_b)\n","\n","W1mt_acc = beta*W1mt_acc + lr*W1gt_orig/batch_size\n","W2mt_acc = beta*W2mt_acc + lr*W2gt_orig/batch_size\n","\n","new_W1 = W1.assign_add(W1mt_acc)\n","new_W2 = W2.assign_add(W2mt_acc)\n","\n","# define the SRM kernels here\n","tau = num_iter/10\n","kernelh = snn.gen_kernel(k_size, hidden_size, tau)\n","kernely = snn.gen_kernel(k_size, num_output, tau)\n","\n","tau = num_iter/100\n","kernely_b = snn.gen_kernel(k_size, num_output, tau)\n","kernelh_b = snn.gen_kernel(k_size, hidden_size, tau)"]},{"cell_type":"markdown","metadata":{"id":"oMv7cKhh6Bls"},"source":["Run the training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLeamABZ5nYo"},"outputs":[],"source":["gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n","pr_fq = 2\n","all_test_accuracies = []\n","all_test_accuracies_acc = []\n","max_acc = 0\n","with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n","    sess.run(tf.global_variables_initializer())\n","    for e in range(num_epochs):\n","        num_batches = data_size//batch_size\n","        print(\"epoch : \", e)\n","        accs = []\n","        rand_perm = rng.permutation(data_size)\n","        X_in_train = X_in_train[rand_perm]\n","        y_in_train = y_in_train[rand_perm]\n","        pick_dropout_prob = np.random.uniform()\n","        dropout_prob2 = 0.3\n","        dropout_prob1 = 0.2\n","        for b in range(num_batches):\n","            #generate dropout mask\n","            p = np.random.uniform(size=N)\n","            p[p<dropout_prob1] = 0\n","            p[p>0] = 1\n","            ps_arr1 = np.zeros((batch_size, 1, num_iter, N))\n","            ps_arr1[:, :, :, p>0] = 1\n","\n","            p = np.random.uniform(size=hidden_size)\n","            p[p<dropout_prob2] = 0\n","            p[p>0] = 1\n","            ps_arr2 = np.zeros((batch_size, 1, num_iter, hidden_size))\n","            ps_arr2[:, :, :, p>0] = 1\n","\n","            in_batch = np.float64(X_in_train[b*batch_size:(b+1)*batch_size])\n","            label_batch = np.float64(y_in_train[b*batch_size:(b+1)*batch_size])\n","            feeds = {X_pl: in_batch, s_label: label_batch, tf_kernelh: kernelh, tf_kernelh_b: kernelh_b, tf_kernely: kernely, tf_kernely_b: kernely_b, dropout_mask1: ps_arr1, dropout_mask2: ps_arr2, keep_prob1: 1-dropout_prob1, keep_prob2: 1-dropout_prob2}\n","            out_y,_,_ = sess.run([s_out_y_acc, new_W1, new_W2], feed_dict=feeds)\n","\n","            out_means = np.mean(out_y, axis=(1,2))\n","            label_means = np.mean(label_batch, axis=(1,2))\n","            out_labels = np.argmax(out_means, axis=1)\n","            orig_labels = np.argmax(label_means, axis=1)\n","            batch_acc = np.mean(np.float32(out_labels==orig_labels))\n","            accs.append(batch_acc)\n","        if e%pr_fq == 0:\n","            test_accs = []\n","            test_accs_acc = []\n","            num_test_batches = data_size_test//batch_size\n","            for c in range(num_test_batches):\n","                in_batch = np.float64(X_in_test[c*batch_size:(c+1)*batch_size])\n","                label_batch = np.float64(y_in_test[c*batch_size:(c+1)*batch_size])\n","                ps_arr_test1 = np.ones((batch_size, 1, num_iter, N))\n","                ps_arr_test2 = np.ones((batch_size, 1, num_iter, hidden_size))\n","                feeds = {X_pl: in_batch, tf_kernelh: kernelh, tf_kernely: kernely, dropout_mask1: ps_arr_test1, dropout_mask2: ps_arr_test2, keep_prob1: 1, keep_prob2: 1}\n","                out_h,out_y = sess.run([s_out_h_acc, s_out_y_acc], feed_dict=feeds)\n","\n","                out_means = np.mean(out_y, axis=(1,2))\n","                label_means = np.mean(label_batch, axis=(1,2))\n","                out_labels = np.argmax(out_means, axis=1)\n","                orig_labels = np.argmax(label_means, axis=1)\n","                batch_acc = np.mean(np.float32(out_labels==orig_labels))\n","                test_accs.append(batch_acc)\n","            test_accuracy = np.mean(batch_size*np.array(test_accs))/batch_size\n","            all_test_accuracies.append(test_accuracy)\n","            print(\"test accuracy : \", test_accuracy)\n","        epoch_accuracy = np.mean(batch_size*np.array(accs))/batch_size\n","        print(\"accuracy : \", epoch_accuracy)\n","    final_W1 = W1.eval()\n","    final_W2 = W2.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmd33zy2gd8a"},"outputs":[],"source":["\n","plt.figure()\n","plt.plot(np.arange(0,2*len(all_test_accuracies),2), 100*np.array(all_test_accuracies))\n","plt.xlabel('epochs')\n","plt.ylabel('test accuracy (%)')\n","plt.show()\n","print(\"n_iter : \", num_iter)\n","print(\"max test acc : \" + str(np.max(np.array(all_test_accuracies))))\n","print(\"last test acc : \" + str(all_test_accuracies[-1]))\n","print(\"H1 : \", hidden_size)\n","#print(all_test_accuracies)\n","running_avg_acc = np.zeros_like(np.array(all_test_accuracies))\n","running_std_acc = np.zeros_like(np.array(all_test_accuracies))\n","for i in range(len(all_test_accuracies)):\n","    if i<9:\n","        running_avg_acc[i] = np.mean(np.array(all_test_accuracies)[0:i+1])\n","        running_std_acc[i] = np.std(np.array(all_test_accuracies)[0:i+1])\n","    else:\n","        running_avg_acc[i] = np.mean(np.array(all_test_accuracies)[i-9:i+1])\n","        running_std_acc[i] = np.std(np.array(all_test_accuracies)[i-9:i+1])\n","\n","plt.figure()\n","plt.plot(np.arange(0,2*len(all_test_accuracies),2), 100*running_avg_acc)\n","plt.xlabel('epochs')\n","plt.ylabel('running average (last 10) of test accuracy (%)')\n","\n","print('self iter - ', self_iter)\n","print('dropout 1 - ', dropout_prob1)\n","print('dropout 2 - ', dropout_prob2)\n","print('raw test accuracies')\n","print(all_test_accuracies)\n","print('running avg test accuracies')\n","print(running_avg_acc)\n","print('running std test accuracies')\n","print(running_std_acc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tobOmuGQgyuc"},"outputs":[],"source":["print(\"number of input  spikes per sample : \", np.sum(in_batch)/batch_size)\n","print(\"number of hidden spikes per sample : \", np.sum(out_h)/batch_size)\n","print(\"number of output spikes per sample : \", np.sum(out_y)/batch_size)\n","print(\"number of target spikes per sample : \", np.sum(label_batch)/batch_size)\n","#print(\"number of output spikes per sample w/o inh : \", np.sum(out_y_orig)/100)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}