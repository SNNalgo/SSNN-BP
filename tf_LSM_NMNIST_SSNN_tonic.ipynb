{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oHIwq83DXiHF"},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import pickle\n","import tensorflow as tf\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"wV0ydSFuqGNL"},"source":["Load Data"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"qbbKrwA2pNzG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tonic --quiet\n","!pip install snntorch --quiet"],"metadata":{"id":"su4H8oVg0nSk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchvision\n","import tonic.transforms as transforms\n","from torch.utils.data import DataLoader\n","import tonic\n","from tonic import CachedDataset\n","\n","sensor_size = tonic.datasets.NMNIST.sensor_size\n","\n","# Denoise removes isolated, one-off events\n","# time_window\n","frame_transform = transforms.Compose([transforms.Denoise(filter_time=3000), \n","                                      transforms.ToFrame(sensor_size=sensor_size, \n","                                                         time_window=1000)\n","                                     ])\n","\n","trainset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=True)\n","testset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=False)\n","\n","transform = tonic.transforms.Compose([torch.from_numpy])\n","\n","cached_trainset = CachedDataset(trainset, transform=transform, cache_path='./cache/nmnist/train')\n","cached_testset = CachedDataset(testset, cache_path='./cache/nmnist/test')\n","\n","batch_size = 100\n","train_dl = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(), shuffle=True)\n","test_dl = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors())"],"metadata":{"id":"jMuXfmuq1Mwt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7bct5PNyQWki"},"outputs":[],"source":["num_output = 10\n","in_size = 2*34*34\n","num_iter_orig = 300 # max num_iter = 300, we can try smaller values\n","num_iter = 300"]},{"cell_type":"code","source":["event_tensor, target = next(iter(train_dl))\n","print(event_tensor.shape)"],"metadata":{"id":"xBMNngFS_zZ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZDfRmZD-qJi2"},"source":["Define Weight matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGSuqEDuqEU_"},"outputs":[],"source":["Nx = 10\n","Ny = 10\n","Nz = 10\n","N = Nx*Ny*Nz\n","\n","inW = 32\n","inh_fac = 1\n","in_conn_density = 0.15\n","\n","LqW = 2\n","inh_fr = 0.2\n","lam = 9\n","\n","W_lsm = np.zeros((N,N))\n","W_in = np.zeros((in_size,N))\n","W_lsm_mask = np.ones((N,N))\n","\n","in_conn_range = np.int32(N*in_conn_density)\n","for i in range(in_size):\n","  input_perm_i = np.arange(N)\n","  np.random.shuffle(input_perm_i)\n","  pos_conn = input_perm_i[:in_conn_range]\n","  neg_conn = input_perm_i[-in_conn_range:]\n","  W_in[i,pos_conn] = inW\n","  W_in[i,neg_conn] = -1*inh_fac*inW\n","\n","input_perm = np.arange(N)\n","np.random.shuffle(input_perm) # first 0.2*N indices are inhibitory\n","inh_range = np.int32(inh_fr*N) # indices 0 to inh_range-1 are inhibitory\n","\n","for i in range(N):\n","  posti = input_perm[i] # input_perm[i] is the post-neuron index\n","  zi = posti//(Nx*Ny)\n","  yi = (posti-zi*Nx*Ny)//Nx\n","  xi = (posti-zi*Nx*Ny)%Nx\n","  for j in range(N):\n","    prej = input_perm[j] # input_perm[j] is the pre-neuron index\n","    zj = prej//(Nx*Ny)\n","    yj = (prej-zj*Nx*Ny)//Nx\n","    xj = (prej-zj*Nx*Ny)%Nx\n","    D = ((xi-xj)**2 + (yi-yj)**2 + (zi-zj)**2)\n","    if i<inh_range and j<inh_range: # II connection, C = 0.3\n","      P = 0.3*np.exp(-D/lam)\n","      Pu1 = np.random.uniform()\n","      if Pu1<P:\n","        W_lsm[prej,posti] = -LqW\n","    if i<inh_range and j>=inh_range: # EI connection, C = 0.1\n","      P = 0.1*np.exp(-D/lam)\n","      Pu1 = np.random.uniform()\n","      if Pu1<P:\n","        W_lsm[prej,posti] = LqW\n","    if i>=inh_range and j<inh_range: # IE connection, C = 0.05\n","      P = 0.05*np.exp(-D/lam)\n","      Pu1 = np.random.uniform()\n","      if Pu1<P:\n","        W_lsm[prej,posti] = -LqW\n","    if i>=inh_range and j>=inh_range: # EE connection, C = 0.2\n","      P = 0.2*np.exp(-D/lam)\n","      Pu1 = np.random.uniform()\n","      if Pu1<P:\n","        W_lsm[prej,posti] = LqW\n","\n","for i in range(N):\n","  W_lsm[i,i] = 0\n","  W_lsm_mask[i,i] = 0\n","\n","abs_W_lsm = np.abs(W_lsm)\n","\n","print(\"average fan out: \", np.mean(np.sum(abs_W_lsm>0, axis=1)))"]},{"cell_type":"markdown","metadata":{"id":"NTH4Nx0137aV"},"source":["Define Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYlyzYHzwdi0"},"outputs":[],"source":["\n","def run_LSM(Vm, SI, Sin, SinI, eta, eta1, a1, a2, in_W, W, sc_W, th, n_iter):\n","  Sliq = []\n","  for i in range(n_iter):\n","    Vm = (1-eta)*Vm + a1*(tf.matmul(SI, sc_W*W) + tf.matmul(SinI, in_W))\n","    Sout = tf.cast(Vm>=th, dtype=tf.float32)\n","    Vm = Vm*tf.cast(Vm<th, dtype=tf.float32)\n","    SinI = eta1*SinI + a2*Sin[:,i,:]\n","    SI = eta1*SI + a2*Sout\n","    Sliq.append(Sout)\n","  return tf.stack(Sliq, axis=1)\n","\n","def run_LSM_no_recursion(Vm, SI, Sin, SinI, eta, eta1, a1, a2, in_W, W, sc_W, th, n_iter):\n","  Sliq = []\n","  for i in range(n_iter):\n","    Vm = (1-eta)*Vm + a1*tf.matmul(SinI, in_W)\n","    Sout = tf.cast(Vm>=th, dtype=tf.float32)\n","    Vm = Vm*tf.cast(Vm<th, dtype=tf.float32)\n","    SinI = eta1*SinI + a2*Sin[:,i,:]\n","    Sliq.append(Sout)\n","  return tf.stack(Sliq, axis=1)\n","\n","# LSM with STDP in all (LL and IL) connections\n","def run_LSM_full_STDP(Vm, SI, Sin, SinI, spT, spT_in, eta, eta1, a1, a2, in_W, in_W_sign, W, W_sign, W_mask, th, n_iter, batch_s, Nin, Nrev, Ap, An, a3, eta2, dw_in_scale=1, dw_lsm_scale=1):\n","  Sliq = []\n","  for i in range(n_iter):\n","    Vm = (1-eta)*Vm + a1*(tf.matmul(SI, W) + tf.matmul(SinI, in_W))\n","    Sout = tf.cast(Vm>=th, dtype=tf.float32)\n","    Vm = Vm*tf.cast(Vm<th, dtype=tf.float32)\n","    SinI = eta1*SinI + a2*Sin[:,i,:]\n","    SI = eta1*SI + a2*Sout\n","    spT = (1-eta2)*spT + a3*Sout\n","    spT_in = (1-eta2)*spT_in + a3*Sin[:,i,:]\n","\n","    dwp = Ap*tf.matmul(tf.transpose(spT_in), Sout)/batch_s\n","    dwn = -An*tf.matmul(tf.transpose(Sin[:,i,:]), spT)/batch_s\n","    in_W = in_W + dw_in_scale*(dwp + dwn)\n","\n","    #in_W_check = in_W*in_W_sign\n","    #in_W = in_W*tf.cast(in_W_check>=0, dtype=tf.float32)\n","    \n","    dwp = Ap*tf.matmul(tf.transpose(spT), Sout)/batch_s\n","    dwn = -An*tf.matmul(tf.transpose(Sout), spT)/batch_s\n","    W = (W + dw_lsm_scale*(dwp + dwn))*W_mask\n","\n","    W_check = W*W_sign\n","    W = W*tf.cast(W_check>=0, dtype=tf.float32)\n","    \n","    #tau_astro = 100\n","    #w_astro = 0.01\n","\n","    tau_astro = 1*10\n","    eta_astro = 1/tau_astro\n","    w_astro = 1*0.01\n","\n","    #An = An*(1/(1 + 1/tau_astro)) + ((w_astro/tau_astro)/(1 + 1/tau_astro))*(tf.reduce_mean(tf.reduce_sum(Sout, axis=1)) - tf.reduce_mean(tf.reduce_sum(Sin[:,i,:], axis=1))) + ((Ap/tau_astro)/(1 + 1/tau_astro))\n","    An = An*(1-eta_astro) + (w_astro/tau_astro)*(tf.reduce_mean(tf.reduce_sum(Sout, axis=1)) - 2*tf.reduce_mean(tf.reduce_sum(Sin[:,i,:], axis=1))) + Ap/tau_astro\n","    \n","    Sliq.append(Sout)\n","  return tf.stack(Sliq, axis=1), in_W, W, An\n","\n","def run_LSM_full_STDP_exp_avg(Vm, SI, Sin, SinI, spT, spT_in, avSpiking, eta, eta1, a1, a2, in_W, in_W_sign, W, W_sign, W_mask, th, n_iter, batch_s, Nin, Nrev, Ap, An, a3, eta2, avDecay=0.9, dw_in_scale=1, dw_lsm_scale=1):\n","  Sliq = []\n","  \n","  for i in range(n_iter):\n","    Vm = (1-eta)*Vm + a1*(tf.matmul(SI, W) + tf.matmul(SinI, in_W))\n","    Sout = tf.cast(Vm>=th, dtype=tf.float32)\n","    Vm = Vm*tf.cast(Vm<th, dtype=tf.float32)\n","    SinI = eta1*SinI + a2*Sin[:,i,:]\n","    SI = eta1*SI + a2*Sout\n","    spT = (1-eta2)*spT + a3*Sout\n","    spT_in = (1-eta2)*spT_in + a3*Sin[:,i,:]\n","\n","    dwp = Ap*tf.matmul(tf.transpose(spT_in), Sout)/batch_s\n","    dwn = -An*tf.matmul(tf.transpose(Sin[:,i,:]), spT)/batch_s\n","    in_W = in_W + dw_in_scale*(dwp + dwn)\n","\n","    #in_W_check = in_W*in_W_sign\n","    #in_W = in_W*tf.cast(in_W_check>=0, dtype=tf.float32)\n","    \n","    dwp = Ap*tf.matmul(tf.transpose(spT), Sout)/batch_s\n","    dwn = -An*tf.matmul(tf.transpose(Sout), spT)/batch_s\n","    W = (W + dw_lsm_scale*(dwp + dwn))*W_mask\n","\n","    W_check = W*W_sign\n","    W = W*tf.cast(W_check>=0, dtype=tf.float32)\n","    \n","    #tau_astro = 100\n","    #w_astro = 0.01\n","\n","    tau_astro = 1*10\n","    eta_astro = 1/tau_astro\n","    w_astro = 1*0.01\n","\n","    liq_boost_factor = 2\n","\n","    avSpiking = avDecay*avSpiking + (1 - avDecay)*Sin[:,i,:]\n","    An = An*(1-eta_astro) + (w_astro/tau_astro)*(tf.reduce_mean(tf.reduce_sum(Sout, axis=1)) - liq_boost_factor*tf.reduce_mean(tf.reduce_sum(avSpiking, axis=1))) + Ap/tau_astro\n","    \n","    Sliq.append(Sout)\n","  return tf.stack(Sliq, axis=1), in_W, W, An\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2TRP4uBWvh-"},"outputs":[],"source":["print(\"max value in W_in : \", np.max(W_in))\n","print(\"min value in W_in : \", np.min(W_in))\n","\n","print(\"max value in W_lsm : \", np.max(W_lsm))\n","print(\"min value in W_lsm : \", np.min(W_lsm))\n","\n","#abs_W_lsm = np.abs(W_lsm)\n","#low_cut_off = 1\n","#print(\"average fan out: \", np.mean(np.sum(abs_W_lsm>low_cut_off, axis=1)))"]},{"cell_type":"markdown","metadata":{"id":"SxfYl5h7aPm-"},"source":["Run LSM without STDP to generate liquid activations of train and test sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z93ajlxM62mQ"},"outputs":[],"source":["\n","#W_lsm_tf = tf.constant(W_lsm*tf.cast(abs_W_lsm>low_cut_off, dtype=tf.float32), dtype=tf.float32)\n","W_lsm_tf = tf.constant(W_lsm, dtype=tf.float32)\n","W_in_tf = tf.constant(W_in, dtype=tf.float32)\n","\n","dt = 1.0\n","tauV = 16.0\n","tauU = 16.0\n","\n","eta = dt/tauV\n","a1 = dt\n","\n","eta1 = np.exp(-dt/tauU)\n","a2 = 1/tauU\n","\n","th = 20\n","\n","num_partitions = 2\n","LSM_out_train = []\n","LSM_out_test = []\n","\n","y_train = []\n","y_test = []\n","\n","w_scale = 1\n","partition = num_iter//num_partitions\n","start_ind = 0\n","\n","batch_size = 100\n","b = 0\n","train_mean_spikes = 0\n","batch_spikes_i = 0\n","batch_spikes_l = 0\n","for batch in train_dl:\n","  b = b+1\n","  if(b%50==0):\n","    print(\"batches completed: \", b)\n","  Vm_tf = tf.constant(np.zeros((batch_size, N)), dtype=tf.float32)\n","  SI_tf = tf.constant(np.zeros((batch_size, N)), dtype=tf.float32)\n","  SinI_tf = tf.constant(np.zeros((batch_size, in_size)), dtype=tf.float32)\n","  spT_tf = tf.constant(np.zeros((batch_size, N)), dtype=tf.float32)\n","\n","  batch_x = batch[0].numpy()\n","  #batch_x = batch[0].numpy()[:,:,0] + batch[0].numpy()[:,:,1]\n","  if batch_x.shape[0] != batch_size:\n","    continue\n","  in_batch = np.reshape(batch_x[:,:num_iter,:,:,:], (batch_size, num_iter, in_size))\n","  label_batch  = batch[1].numpy()\n","\n","  train_mean_spikes = train_mean_spikes + np.mean(in_batch)\n","\n","  batch_in_full_tf = tf.constant(in_batch, dtype=tf.float32)\n","\n","  S_liq = run_LSM(Vm_tf, SI_tf, batch_in_full_tf, SinI_tf, eta, eta1, a1, a2, W_in_tf, W_lsm_tf, w_scale, th, num_iter)\n","\n","  batch_spikes_i = batch_spikes_i + np.mean(np.sum(in_batch, axis=2), axis=0)\n","  batch_spikes_l = batch_spikes_l + np.mean(np.sum(S_liq.numpy(), axis=2), axis=0)\n","  \n","  S_liq_list = []\n","  for i in range(num_partitions):\n","    S_liq_list.append(tf.reduce_mean(S_liq[:,i*partition:(i+1)*partition,:], axis=1))\n","\n","  S_liq_c = tf.concat(S_liq_list, axis=1)\n","  LSM_out_train.append(S_liq_c.numpy())\n","  y_train.append(label_batch)\n","\n","batch_spikes_i = batch_spikes_i/b\n","batch_spikes_l = batch_spikes_l/b\n","\n","train_mean_spikes = train_mean_spikes/b\n","\n","b = 0\n","test_mean_spikes = 0\n","for batch in test_dl:\n","  b = b+1\n","  Vm_tf = tf.constant(np.zeros((batch_size, N)), dtype=tf.float32)\n","  SI_tf = tf.constant(np.zeros((batch_size, N)), dtype=tf.float32)\n","  SinI_tf = tf.constant(np.zeros((batch_size, in_size)), dtype=tf.float32)\n","  spT_tf = tf.constant(np.zeros((batch_size, N)), dtype=tf.float32)\n","\n","  batch_x = batch[0].numpy()\n","  if batch_x.shape[0] != batch_size:\n","    continue\n","  in_batch = np.reshape(batch_x[:,:num_iter,:,:,:], (batch_size, num_iter, in_size))\n","  label_batch  = batch[1].numpy()\n","\n","  test_mean_spikes = test_mean_spikes + np.mean(in_batch)\n","\n","  batch_in_full_tf = tf.constant(in_batch, dtype=tf.float32)\n","\n","  S_liq = run_LSM(Vm_tf, SI_tf, batch_in_full_tf, SinI_tf, eta, eta1, a1, a2, W_in_tf, W_lsm_tf, w_scale, th, num_iter)\n","  \n","  S_liq_list = []\n","  for i in range(num_partitions):\n","    S_liq_list.append(tf.reduce_mean(S_liq[:,i*partition:(i+1)*partition,:], axis=1))\n","\n","  S_liq_c = tf.concat(S_liq_list, axis=1)\n","  LSM_out_test.append(S_liq_c.numpy())\n","  y_test.append(label_batch)\n","\n","test_mean_spikes = test_mean_spikes/b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Kj-WyBCCBt1"},"outputs":[],"source":["plt.figure()\n","plt.grid()\n","plt.xticks(np.arange(0, 301, 20))\n","plt.yticks(np.arange(0, 501, 50))\n","plt.plot(batch_spikes_i)\n","plt.plot(batch_spikes_l)\n","plt.legend(['input spikes','Liquid spikes'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qALXuelif_94"},"outputs":[],"source":["LSM_out_train_np = np.array(LSM_out_train)\n","LSM_out_train_reshp = np.reshape(LSM_out_train_np, (LSM_out_train_np.shape[0]*LSM_out_train_np.shape[1],LSM_out_train_np.shape[2]))\n","LSM_out_test_np = np.array(LSM_out_test)\n","LSM_out_test_reshp = np.reshape(LSM_out_test_np, (LSM_out_test_np.shape[0]*LSM_out_test_np.shape[1],LSM_out_test_np.shape[2]))\n","\n","y_train_np = np.array(y_train)\n","y_test_np = np.array(y_test)\n","y_train_data = np.int32(np.reshape(y_train_np, (LSM_out_train_np.shape[0]*LSM_out_train_np.shape[1])))\n","y_test_data = np.int32(np.reshape(y_test_np, (LSM_out_test_np.shape[0]*LSM_out_test_np.shape[1])))\n","\n","print(\"mean LSM spiking (train) : \", np.mean(LSM_out_train_np))\n","print(\"mean LSM spiking (test) : \", np.mean(LSM_out_test_np))\n","\n","print(\"mean input spiking (train) : \", train_mean_spikes)\n","print(\"mean input spiking (test) : \", test_mean_spikes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xm76w5c22Zdv"},"outputs":[],"source":["data_path = '/content/drive/My Drive/Colab Notebooks/SSNN-BP/'\n","#np.save(data_path + 'W_in_NMNIST_p2', W_in)\n","#np.save(data_path + 'W_lsm_NMNIST_p2', W_lsm)\n","np.save(data_path + 'train_LSM_out_NMNIST_p2', LSM_out_train_reshp)\n","np.save(data_path + 'train_labels_NMNIST_p2', y_train_data)\n","np.save(data_path + 'test_LSM_out_NMNIST_p2', LSM_out_test_reshp)\n","np.save(data_path + 'test_labels_NMNIST_p2', y_test_data)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyMzwMerfGzYxMIN+vQqdlQJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}