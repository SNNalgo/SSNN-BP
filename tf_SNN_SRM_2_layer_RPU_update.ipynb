{"cells":[{"cell_type":"markdown","metadata":{"id":"soYRyTXKbfVX"},"source":["import stuff"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPzfqnebbRwH"},"outputs":[],"source":["import numpy as np\n","import tensorflow.compat.v1 as tf\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import pickle"]},{"cell_type":"code","source":["#from google.colab import drive\n","#drive.mount('/content/drive')\n","\n","#save_path = '/content/drive/My Drive/Colab Notebooks/SNN_SRM/W_backup/'"],"metadata":{"id":"gdvz9C4-HV7S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"QV8__SjZMk8g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks/SSNN-BP')"],"metadata":{"id":"Ptm8mblHMlg8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GRq3ERFIl_x5"},"source":["tf.nn.depthwise_conv2d will be used to simulate the SNN with a Spike Response Model. For this, it needs input of shape (batch, 1, in_width, in_channels) and filters of shape (1, filter_width, in_channels, 1). The convolution occurs in the dimension of width - which should correspond to simulation steps \n","Describe the model in the following block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwGP-pEYDjL6"},"outputs":[],"source":["tf.disable_v2_behavior()\n","seed = 0\n","rng = np.random.RandomState(seed)\n","tf.set_random_seed(seed)\n","np.seterr(all='raise')\n","\n","# SNN parameters\n","\n","#num_iter = 101\n","#max_in_spikes = 101\n","#max_out_spikes = 51\n","\n","#num_iter = 201\n","#max_in_spikes = 201\n","#max_out_spikes = 101\n","\n","num_iter = 301\n","max_in_spikes = 301\n","max_out_spikes = 151\n","\n","dataset = 'mnist_784'   # full size 70000\n","data_size = 30000  # Training + validation data size\n","data_size_test = 10000  # Test data size\n","\n","data = fetch_openml(dataset)\n","X_full = data.data\n","y_full = np.float32(data.target)\n","X_full = (X_full - X_full.min()) / (X_full.max()+0.000001 - X_full.min())\n","\n","num_images = X_full.shape[0]\n","N = 28 * 28\n","im_size = 28\n","num_output = 10\n","\n","rand_perm = rng.permutation(num_images)\n","X_full = X_full.to_numpy()\n","X_full = X_full[rand_perm]\n","y_full = y_full[rand_perm]\n","\n","if data_size + data_size_test > X_full.shape[0]:\n","    data_size = X_full.shape[0] - data_size_test\n","\n","print(\"total size: \", X_full.shape[0])\n","print(\"train size: \", data_size)\n","print(\"test size: \", data_size_test)\n","\n","X_data = X_full[:data_size]\n","y_data = y_full[:data_size]\n","\n","X_test = X_full[-data_size_test:]\n","y_test = y_full[-data_size_test:]\n","\n","#Fill out the labels\n","y_in_train = np.zeros((X_data.shape[0], 1, num_iter, num_output), dtype=np.int8)\n","y_in_test = np.zeros((X_test.shape[0], 1, num_iter, num_output), dtype=np.int8)\n","\n","for i in range(X_data.shape[0]):\n","    label_ind = np.int32(y_data[i])\n","    spikes = np.random.uniform(size=num_iter)\n","    spikes[spikes<(max_out_spikes/num_iter)] = 1\n","    spikes[spikes<1] = 0\n","    y_in_train[i,0,:,label_ind] = spikes\n","\n","for i in range(X_test.shape[0]):\n","    label_ind = np.int32(y_test[i])\n","    spikes = np.random.uniform(size=num_iter)\n","    spikes[spikes<(max_out_spikes/num_iter)] = 1\n","    spikes[spikes<1] = 0\n","    y_in_test[i,0,:,label_ind] = spikes"]},{"cell_type":"markdown","source":["Fill out the spike data"],"metadata":{"id":"7CylBph9UBcZ"}},{"cell_type":"code","source":["batch_size = 100\n","\n","X_in_train = np.zeros((X_data.shape[0], 1, num_iter, X_data.shape[1]), dtype=np.int8)\n","X_in_test = np.zeros((X_test.shape[0], 1, num_iter, X_test.shape[1]), dtype=np.int8)\n","\n","num_batches = data_size//batch_size\n","num_batches_test = data_size_test//batch_size\n","for b in range(num_batches):\n","  if(b%50==0):\n","    print(\"completed: \", b/num_batches)\n","  batch_data = X_data[b*batch_size:(b+1)*batch_size]*(max_in_spikes/num_iter)\n","  batch_reshp = np.reshape(batch_data, (batch_size, -1, 1, 1)) #shape=(batch_size, N, 1, 1)\n","  batch_data_tiled = np.tile(batch_reshp, (1, 1, num_iter, 1)) #shape=(batch_size, N, num_iter, 1)\n","  batch_data_trans = np.transpose(batch_data_tiled, (0,3,2,1)) #shape=(batch_size, 1, num_iter, N)\n","  batch_in = np.random.uniform(size=(batch_size, 1, num_iter, N))\n","  batch_in[batch_in<batch_data_trans] = 1\n","  batch_in[batch_in<1] = 0\n","  X_in_train[b*batch_size:(b+1)*batch_size] = batch_in\n","\n","for b in range(num_batches_test):\n","  batch_data = X_test[b*batch_size:(b+1)*batch_size]*(max_in_spikes/num_iter)\n","  batch_reshp = np.reshape(batch_data, (batch_size, -1, 1, 1))\n","  batch_data_tiled = np.tile(batch_reshp, (1, 1, num_iter, 1))\n","  batch_data_trans = np.transpose(batch_data_tiled, (0,3,2,1))\n","  batch_in = np.random.uniform(size=(batch_size, 1, num_iter, N))\n","  batch_in[batch_in<batch_data_trans] = 1\n","  batch_in[batch_in<1] = 0\n","  X_in_test[b*batch_size:(b+1)*batch_size] = batch_in\n"],"metadata":{"id":"_lRCkezKP_PL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WjUHWT6NMfZW"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","sample = 500\n","image = X_data[sample].reshape((28,28))# plot the sample\n","fig = plt.figure\n","plt.imshow(image, cmap='gray')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZdFZEPRbcuJ"},"outputs":[],"source":["import RPU_SNN as snn\n","\n","batch_size = 50\n","num_epochs = 11\n","lr = 0.005\n","beta = 0.9\n","eps = 1e-8\n","k_size = num_iter\n","hidden_size = 1280\n","th_val_h = 5\n","th_val_y = 5\n","\n","self_iter = 5\n","#self_iter = 0\n","\n","# Initialize Placeholders (X_in, labels, dropout_masks, kernels)\n","keep_prob1 = tf.placeholder('float', shape=())\n","keep_prob2 = tf.placeholder('float', shape=())\n","tf_kernelh = tf.placeholder('float', [1, k_size, hidden_size, 1])\n","tf_kernelh_b = tf.placeholder('float', [1, k_size, hidden_size, 1])\n","tf_kernely = tf.placeholder('float', [1, k_size, num_output, 1])\n","tf_kernely_b = tf.placeholder('float', [1, k_size, num_output, 1])\n","dropout_mask1 = tf.placeholder('float', [batch_size,1,num_iter,N])\n","dropout_mask2 = tf.placeholder('float', [batch_size,1,num_iter,hidden_size])\n","X_pl = tf.placeholder('float', [batch_size, 1, num_iter, N])\n","s_label = tf.placeholder('float', [batch_size, 1, num_iter, num_output])\n","\n","# Initialize Weights (He initialization)\n","W1 = tf.Variable(tf.random_normal((N,hidden_size), stddev=np.sqrt(4/N)), trainable=False)\n","W2 = tf.Variable(tf.random_normal((hidden_size,num_output), stddev=np.sqrt(4/hidden_size)), trainable=False)\n","W_inh = tf.Variable(tf.matrix_set_diag(1.0*tf.ones((num_output, num_output)), tf.zeros(num_output)), trainable=False)\n","W1mt = tf.Variable(tf.zeros((N,hidden_size)), trainable=False)\n","W2mt = tf.Variable(tf.zeros((hidden_size, num_output)), trainable=False)\n","\n","W1mt_acc = tf.Variable(W1mt)\n","W2mt_acc = tf.Variable(W2mt)\n","\n","# MOMENTUM\n","#W1_acc = W1\n","#W2_acc = W2\n","\n","# NESTEROV ACCELERATED GRADIENT\n","W1_acc = W1 + beta*W1mt_acc\n","W2_acc = W2 + beta*W2mt_acc\n","\n","X_masked, s_out_h_acc = snn.forward_pass(X_pl, dropout_mask1, W1_acc, th_val_h, tf_kernelh)\n","s_out_h_masked, s_out_y_acc = snn.y_WTA(s_out_h_acc, dropout_mask2, W2_acc, th_val_y, tf_kernely, W_inh, self_iter)\n","\n","del_p_y, del_m_y, W2gt_orig = snn.finalW_update(s_out_y_acc, s_out_h_masked, s_label, th_val_y, tf_kernely_b)\n","del_p_h, del_m_h, W1gt_orig = snn.innerW_update(s_out_h_masked, X_masked, del_p_y, del_m_y, W2_acc, th_val_h, tf_kernelh_b)\n","\n","W1mt_acc = beta*W1mt_acc + lr*W1gt_orig\n","W2mt_acc = beta*W2mt_acc + lr*W2gt_orig\n","\n","new_W1 = W1.assign_add(W1mt_acc)\n","new_W2 = W2.assign_add(W2mt_acc)\n","\n","# define the SRM kernels here\n","tau = num_iter/10\n","kernelh = snn.gen_kernel(k_size, hidden_size, tau)\n","kernely = snn.gen_kernel(k_size, num_output, tau)\n","\n","tau = num_iter/100\n","kernely_b = snn.gen_kernel(k_size, num_output, tau)\n","kernelh_b = snn.gen_kernel(k_size, hidden_size, tau)"]},{"cell_type":"markdown","metadata":{"id":"oMv7cKhh6Bls"},"source":["Run the training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLeamABZ5nYo"},"outputs":[],"source":["gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n","pr_fq = 2\n","all_test_accuracies = []\n","all_test_accuracies_acc = []\n","\n","with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n","    sess.run(tf.global_variables_initializer())\n","    for e in range(num_epochs):\n","        num_batches = data_size//batch_size\n","        print(\"epoch : \", e)\n","        accs = []\n","        rand_perm = rng.permutation(data_size)\n","        X_in_train = X_in_train[rand_perm]\n","        y_in_train = y_in_train[rand_perm]\n","        pick_dropout_prob = np.random.uniform()\n","        dropout_prob2 = 0.3\n","        dropout_prob1 = 0.2\n","        for b in range(num_batches):\n","            #generate dropout mask\n","            p = np.random.uniform(size=N)\n","            p[p<dropout_prob1] = 0\n","            p[p>0] = 1\n","            ps_arr1 = np.zeros((batch_size, 1, num_iter, N))\n","            ps_arr1[:, :, :, p>0] = 1\n","\n","            p = np.random.uniform(size=hidden_size)\n","            p[p<dropout_prob2] = 0\n","            p[p>0] = 1\n","            ps_arr2 = np.zeros((batch_size, 1, num_iter, hidden_size))\n","            ps_arr2[:, :, :, p>0] = 1\n","            \n","            in_batch = np.float64(X_in_train[b*batch_size:(b+1)*batch_size])\n","            label_batch = np.float64(y_in_train[b*batch_size:(b+1)*batch_size])\n","            feeds = {X_pl: in_batch, s_label: label_batch, tf_kernelh: kernelh, tf_kernelh_b: kernelh_b, tf_kernely: kernely, tf_kernely_b: kernely_b, dropout_mask1: ps_arr1, dropout_mask2: ps_arr2, keep_prob1: 1-dropout_prob1, keep_prob2: 1-dropout_prob2}\n","            out_y,_,_ = sess.run([s_out_y_acc, new_W1, new_W2], feed_dict=feeds)\n","\n","            out_means = np.mean(out_y, axis=(1,2))\n","            label_means = np.mean(label_batch, axis=(1,2))\n","            out_labels = np.argmax(out_means, axis=1)\n","            orig_labels = np.argmax(label_means, axis=1)\n","            batch_acc = np.mean(np.float32(out_labels==orig_labels))\n","            accs.append(batch_acc)\n","        if e%pr_fq == 0:\n","            test_accs = []\n","            test_accs_acc = []\n","            num_test_batches = data_size_test//batch_size\n","            for c in range(num_test_batches):\n","                in_batch = np.float64(X_in_test[c*batch_size:(c+1)*batch_size])\n","                label_batch = np.float64(y_in_test[c*batch_size:(c+1)*batch_size])\n","                ps_arr_test1 = np.ones((batch_size, 1, num_iter, N))\n","                ps_arr_test2 = np.ones((batch_size, 1, num_iter, hidden_size))\n","                feeds = {X_pl: in_batch, tf_kernelh: kernelh, tf_kernely: kernely, dropout_mask1: ps_arr_test1, dropout_mask2: ps_arr_test2, keep_prob1: 1, keep_prob2: 1}\n","                out_h,out_y = sess.run([s_out_h_acc, s_out_y_acc], feed_dict=feeds)\n","                \n","                out_means = np.mean(out_y, axis=(1,2))\n","                label_means = np.mean(label_batch, axis=(1,2))\n","                out_labels = np.argmax(out_means, axis=1)\n","                orig_labels = np.argmax(label_means, axis=1)\n","                batch_acc = np.mean(np.float32(out_labels==orig_labels))\n","                test_accs.append(batch_acc)\n","            test_accuracy = np.mean(batch_size*np.array(test_accs))/batch_size\n","            all_test_accuracies.append(test_accuracy)\n","            print(\"test accuracy : \", test_accuracy)\n","        epoch_accuracy = np.mean(batch_size*np.array(accs))/batch_size\n","        print(\"accuracy : \", epoch_accuracy)\n","    final_W1 = W1.eval()\n","    final_W2 = W2.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmd33zy2gd8a"},"outputs":[],"source":["plt.figure()\n","plt.plot(np.arange(0,2*len(all_test_accuracies),2), 100*np.array(all_test_accuracies))\n","plt.xlabel('epochs')\n","plt.ylabel('test accuracy (%)')\n","plt.show()\n","print(\"n_iter : \", num_iter)\n","print(\"max test acc : \" + str(np.max(np.array(all_test_accuracies))))\n","print(\"last test acc : \" + str(all_test_accuracies[-1]))\n","print(\"H1 : \", hidden_size)\n","#print(all_test_accuracies)\n","running_avg_acc = np.zeros_like(np.array(all_test_accuracies))\n","running_std_acc = np.zeros_like(np.array(all_test_accuracies))\n","for i in range(len(all_test_accuracies)):\n","    if i<9:\n","        running_avg_acc[i] = np.mean(np.array(all_test_accuracies)[0:i+1])\n","        running_std_acc[i] = np.std(np.array(all_test_accuracies)[0:i+1])\n","    else:\n","        running_avg_acc[i] = np.mean(np.array(all_test_accuracies)[i-9:i+1])\n","        running_std_acc[i] = np.std(np.array(all_test_accuracies)[i-9:i+1])\n","\n","plt.figure()\n","plt.plot(np.arange(0,2*len(all_test_accuracies),2), 100*running_avg_acc)\n","plt.xlabel('epochs')\n","plt.ylabel('running average (last 10) of test accuracy (%)')\n","\n","print('self iter - ', self_iter)\n","print('dropout 1 - ', dropout_prob1)\n","print('dropout 2 - ', dropout_prob2)\n","print('raw test accuracies')\n","print(all_test_accuracies)\n","print('running avg test accuracies')\n","print(running_avg_acc)\n","print('running std test accuracies')\n","print(running_std_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tobOmuGQgyuc"},"outputs":[],"source":["print(\"number of input  spikes per sample : \", np.sum(in_batch)/batch_size)\n","print(\"number of hidden spikes per sample : \", np.sum(out_h)/batch_size)\n","print(\"number of output spikes per sample : \", np.sum(out_y)/batch_size)\n","print(\"number of target spikes per sample : \", np.sum(label_batch)/batch_size)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}